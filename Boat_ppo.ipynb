{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonorsilva/Reinforcement-learning/blob/main/Boat_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "D1Dsx1AON5cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isInBounds(x, y, width, height):\n",
        "    if x>=0 and x<height and y>=0 and y<width:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        ""
      ],
      "metadata": {
        "id": "HZwe4pClN5Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment():\n",
        "    def env_init(self, env_info={}):\n",
        "        reward = None\n",
        "        self.grid_orig=env_info.get('grid')\n",
        "        self.grid=self.grid_orig.copy()\n",
        "        termination = None\n",
        "        self.start=env_info.get('start')\n",
        "        self.goal=env_info.get('end')\n",
        "        self.grid_h=env_info.get('grid_h')\n",
        "        self.grid_w=env_info.get('grid_w')\n",
        "        self.size=env_info.get('size')\n",
        "\n",
        "        #self.direction=math.atan2(self.goal[1]-self.agent_loc[1],self.goal[0]-self.agent_loc[0])\n",
        "\n",
        "        #self.rot=env_info.get('rot')   #radious\n",
        "        #self.vel=0\n",
        "\n",
        "        self.maxdist=(abs(self.grid_h)+abs(self.grid_w))\n",
        "        self.reward_state_term = (reward, None, None, termination)\n",
        "\n",
        "\n",
        "\n",
        "    def env_start(self):\n",
        "        reward=0\n",
        "        self.agent_loc = (np.random.randint(self.grid_w),np.random.randint(self.grid_h))\n",
        "        self.goal=(np.random.randint(self.grid_w),np.random.randint(self.grid_h))\n",
        "        self.sine=self.agent_loc[0]-self.goal[0]\n",
        "        #math.asin((self.agent_loc[0]-self.goal[0])/self.hipotenusa)\n",
        "        self.cosi=self.agent_loc[1]-self.goal[1]\n",
        "        #math.acos((self.agent_loc[1]-self.goal[1])/self.hipotenusa)\n",
        "\n",
        "        self.grid[self.goal[0]][self.goal[1]]=255\n",
        "        #divide by pi to normalize value\n",
        "        termination = False\n",
        "        x, y = self.agent_loc\n",
        "        if y==self.goal[1] and x==self.goal[0]:\n",
        "            reward=1\n",
        "            termination=True\n",
        "\n",
        "        self.reward_state_term = (reward,self.sine,self.cosi, termination)\n",
        "\n",
        "        return (self.sine,self.cosi,termination)\n",
        "\n",
        "    def env_step(self,action):   #actions: [up,left,down,right]\n",
        "        x, y = self.agent_loc\n",
        "\n",
        "        if action==0:\n",
        "            x=x-1\n",
        "        elif action == 1:\n",
        "            y = y - 1\n",
        "        elif action == 2:\n",
        "            x = x + 1\n",
        "        elif action == 3:\n",
        "            y = y + 1\n",
        "\n",
        "\n",
        "        terminal = False\n",
        "\n",
        "        reward=0  #-((abs(x-self.goal[0])+abs(y-self.goal[1]))/(self.grid_h+self.grid_w))*100\n",
        "\n",
        "        if not isInBounds(x, y, self.grid_w, self.grid_h):\n",
        "            x, y = self.agent_loc\n",
        "            reward=-1\n",
        "            terminal=True\n",
        "\n",
        "        if self.grid[x][y]==0:\n",
        "            x, y = self.agent_loc\n",
        "            reward=0\n",
        "\n",
        "        if y==self.goal[1] and x==self.goal[0]:\n",
        "            reward=1\n",
        "            terminal=True\n",
        "\n",
        "        self.agent_loc = [x, y]\n",
        "\n",
        "        self.sine=self.agent_loc[0]-self.goal[0]\n",
        "        self.cosi=self.agent_loc[1]-self.goal[1]\n",
        "\n",
        "\n",
        "        self.reward_state_term = (reward,self.sine,self.cosi, terminal)\n",
        "\n",
        "        return self.reward_state_term\n",
        "\n",
        "    def env_restart(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "        self.agent_loc = self.start\n",
        "        self.grid=self.grid_orig.copy()"
      ],
      "metadata": {
        "id": "TxjKhLm1N5XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#class Actor_Critic(nn.Module):\n",
        "\n",
        "#    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "#        super(Actor_Critic, self).__init__()\n",
        "#        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        #action\n",
        "        #self.fc1a = nn.Linear(101 , 64)\n",
        "        #self.fc2a = nn.Linear(64, 32)\n",
        "#        self.fc3a = nn.Linear(2, 32)\n",
        "#        self.fc4a = nn.Linear(32, action_size)\n",
        "#        self.sig=nn.Softmax(dim=1)\n",
        "\n",
        "        #value\n",
        "        #self.fc1v = nn.Linear(101 , 64)\n",
        "        #self.fc2v = nn.Linear(64, 32)\n",
        "#        self.fc3v = nn.Linear(2, 32)\n",
        "#        self.fc4v = nn.Linear(32, 1)\n",
        "\n",
        "\n",
        "#    def forward(self, sine,cosi):\n",
        "#        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "\n",
        "\n",
        "#        x = torch.cat([sine,cosi],dim=1)\n",
        "        #x=self.norm(x)\n",
        "\n",
        "        #action\n",
        "#        aux=F.sigmoid(self.fc3a(x))\n",
        "#        aux=F.sigmoid(self.fc4a(aux))\n",
        "#        probs=self.sig(aux)\n",
        "        #logits=torch.logit(aux)\n",
        "        #print(logits)\n",
        "#        dist = torch.distributions.Categorical(probs=probs)\n",
        "\n",
        "#        action = dist.sample()\n",
        "\n",
        "#        log_prob = dist.log_prob(action).unsqueeze(-1)\n",
        "#        entropy = dist.entropy().unsqueeze(-1)\n",
        "\n",
        "        #value\n",
        "#        x=F.tanh(self.fc3v(x))\n",
        "#        v=F.tanh(self.fc4v(x))\n",
        "\n",
        "#        return action,v,log_prob,entropy,probs\n",
        "\n",
        "#    def action(self,sine,cosi,action):\n",
        " #       \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "\n",
        "\n",
        "#        x = torch.cat([sine,cosi],dim=1)\n",
        "        #x=self.norm(x)\n",
        "\n",
        "        #action\n",
        "#        aux=F.sigmoid(self.fc3a(x))\n",
        "#        aux=F.sigmoid(self.fc4a(aux))\n",
        "#        probs=self.sig(aux)\n",
        "        #logits=torch.logit(aux)\n",
        "        #print(logits)\n",
        "#        dist = torch.distributions.Categorical(probs=probs)\n",
        "\n",
        "\n",
        "#        log_prob = dist.log_prob(action).unsqueeze(-1)\n",
        "#        entropy = dist.entropy().unsqueeze(-1)\n",
        "\n",
        "        #value\n",
        "#        x=F.tanh(self.fc3v(x))\n",
        "#        v=F.tanh(self.fc4v(x))\n",
        "\n",
        "#        return action,v,log_prob,entropy,probs\n"
      ],
      "metadata": {
        "id": "BQ52j4fiN5Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        #action\n",
        "        #self.fc1a = nn.Linear(101 , 64)\n",
        "        #self.fc2a = nn.Linear(64, 32)\n",
        "        self.norm=nn.BatchNorm1d(2)\n",
        "        self.fc3a = nn.Linear(2, 32)\n",
        "        self.fc4a = nn.Linear(32, action_size)\n",
        "        self.sig=nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, sine,cosi):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "\n",
        "\n",
        "        x = torch.cat([sine,cosi],dim=1)\n",
        "        #x=self.norm(x)\n",
        "\n",
        "        #action\n",
        "        x=self.norm(x)\n",
        "        aux=F.sigmoid(self.fc3a(x))\n",
        "        aux=F.sigmoid(self.fc4a(aux))\n",
        "        probs=self.sig(aux)\n",
        "        #logits=torch.logit(aux)\n",
        "        #print(logits)\n",
        "        dist = torch.distributions.Categorical(probs=probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "\n",
        "        #log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
        "        #print(log_prob)\n",
        "        entropy = dist.entropy().unsqueeze(-1)\n",
        "        log_prob=torch.index_select(probs,1,action).squeeze(-1)\n",
        "        #print(log_prob)\n",
        "        return action,log_prob,entropy,probs\n",
        "\n",
        "    def action(self,sine,cosi,action):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "\n",
        "\n",
        "        x = torch.cat([sine,cosi],dim=1)\n",
        "        #x=self.norm(x)\n",
        "\n",
        "        #action\n",
        "        x=self.norm(x)\n",
        "        aux=F.sigmoid(self.fc3a(x))\n",
        "        aux=F.sigmoid(self.fc4a(aux))\n",
        "        probs=self.sig(aux)\n",
        "        #logits=torch.logit(aux)\n",
        "        #print(logits)\n",
        "        dist = torch.distributions.Categorical(probs=probs)\n",
        "\n",
        "\n",
        "        #log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
        "\n",
        "        log_prob=torch.index_select(probs,1,action).squeeze(-1)\n",
        "\n",
        "        entropy = dist.entropy().unsqueeze(-1)\n",
        "\n",
        "        return action,log_prob,entropy,probs\n"
      ],
      "metadata": {
        "id": "N3XGOdiCN5Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        #value\n",
        "        #self.fc1v = nn.Linear(101 , 64)\n",
        "        #self.fc2v = nn.Linear(64, 32)\n",
        "        self.norm=nn.BatchNorm1d(2)\n",
        "        self.fc3v = nn.Linear(2, 32)\n",
        "        self.fc4v = nn.Linear(32, 1)\n",
        "\n",
        "\n",
        "    def forward(self, sine,cosi):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "\n",
        "        x = torch.cat([sine,cosi],dim=1)\n",
        "        #x=self.norm(x)\n",
        "\n",
        "        #value\n",
        "        x=self.norm(x)\n",
        "        x=F.tanh(self.fc3v(x))\n",
        "        v=F.tanh(self.fc4v(x))\n",
        "\n",
        "        return v\n",
        "\n"
      ],
      "metadata": {
        "id": "cr8Csw4LN5Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, Adadelta, RMSprop\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "\n",
        "ENT_DECAY = 0.995\n",
        "\n",
        "class PPO:\n",
        "    \"\"\"\n",
        "        This is the PPO class we will use as our model in main.py\n",
        "    \"\"\"\n",
        "    def __init__(self,env, state_size,action_size, **hyperparameters):\n",
        "        \"\"\"\n",
        "            Initializes the PPO model, including hyperparameters.\n",
        "            Parameters:\n",
        "                policy_class - the policy class to use for our actor/critic networks.\n",
        "                env - the environment to train on.\n",
        "                hyperparameters - all extra arguments passed into PPO that should be hyperparameters.\n",
        "            Returns:\n",
        "                None\n",
        "        \"\"\"\n",
        "        # Make sure the environment is compatible with our code\n",
        "        # Initialize hyperparameters for training with PPO\n",
        "        self._init_hyperparameters(hyperparameters)\n",
        "\n",
        "        # Extract environment information\n",
        "        self.env=env\n",
        "        self.obs_dim = state_size\n",
        "        self.act_dim = action_size\n",
        "\n",
        "\n",
        "         # Initialize actor and critic networks\n",
        "        self.Actor = Actor(self.obs_dim, self.act_dim,30)\n",
        "        self.Critic = Critic(self.obs_dim, self.act_dim,30)\n",
        "        #self.critic = Critic(self.obs_dim, self.act_dim,30)\n",
        "\n",
        "\n",
        "        # Initialize optimizers for actor and critic\n",
        "        self.lr_inicial=self.lr\n",
        "        self.b_inital=0.9\n",
        "        #self.optim = RMSprop(self.Actor_Critic.parameters(), lr=self.lr,eps= 1e-5,alpha=0.9,momentum=self.b_inital)\n",
        "        self.optima = Adam(self.Actor.parameters(), lr=self.lr,eps= 1e-5) #,betas=(0.8,0.999))\n",
        "        self.optimc = Adam(self.Critic.parameters(), lr=self.lr,eps= 1e-5)\n",
        "\n",
        "        self.entropy_w=0.0\n",
        "        self.critic_w=1\n",
        "        self.target_kl=None\n",
        "\n",
        "        self.avg_ret=[]\n",
        "        self.random=1\n",
        "\n",
        "\n",
        "        # Initialize the covariance matrix used to query the actor for actions\n",
        "        self.batch=25\n",
        "\n",
        "        # This logger will help us with printing out summaries of each iteration\n",
        "        self.logger = {\n",
        "            'delta_t': time.time(),\n",
        "            't_so_far': 0,          # timesteps so far\n",
        "            'i_so_far': 0,          # iterations so far\n",
        "            'batch_lens': [],       # episodic lengths in batch\n",
        "            'batch_rews': [],       # episodic returns in batch\n",
        "            'actor_losses': [],     # losses of actor network in current iteration\n",
        "            'critic_losses': [],\n",
        "            'entropy_losses': [],\n",
        "            'approx_kl_divs': [],\n",
        "        }\n",
        "\n",
        "    def learn(self, total_timesteps):\n",
        "        \"\"\"\n",
        "            Train the actor and critic networks. Here is where the main PPO algorithm resides.\n",
        "            Parameters:\n",
        "                total_timesteps - the total number of timesteps to train for\n",
        "            Return:\n",
        "                None\n",
        "        \"\"\"\n",
        "        print(f\"Learning... Running {self.max_timesteps_per_episode} timesteps per episode, \", end='')\n",
        "        print(f\"{self.timesteps_per_batch} timesteps per batch for a total of {total_timesteps} timesteps\")\n",
        "        t_so_far = 0 # Timesteps simulated so far\n",
        "        i_so_far = 0 # Iterations ran so far\n",
        "        while i_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
        "            # Autobots, roll out (just kidding, we're collecting our batch simulations here)\n",
        "            batch_sine, batch_cosi, batch_acts, batch_log_probs, batch_rtgs, batch_lens, dones, batch_advs = self.rollout()                     # ALG STEP 3\n",
        "\n",
        "\n",
        "            # Calculate how many timesteps we collected this batch\n",
        "            t_so_far += np.sum(batch_lens)\n",
        "\n",
        "            # Increment the number of iterations\n",
        "            i_so_far += 1\n",
        "\n",
        "            # Logging timesteps so far and iterations so far\n",
        "            self.logger['t_so_far'] = t_so_far\n",
        "            self.logger['i_so_far'] = i_so_far\n",
        "\n",
        "            # Calculate advantage at k-th iteration\n",
        "            V, _ ,_= self.evaluate(batch_sine, batch_cosi, batch_acts)\n",
        "\n",
        "            #_,Vnext,_,_ = self.actor_critic(batch_obs_next)\n",
        "\n",
        "            #batch_rtgs=batch_rtgs+self.gamma*Vnext.detach()*dones\n",
        "\n",
        "            print('return', np.sort(batch_rtgs.detach().numpy().reshape(1,-1)[0])[-10:])\n",
        "            print('return', np.sort(batch_rtgs.detach().numpy().reshape(1,-1)[0])[:10])\n",
        "            #VV=Vnext.cpu().detach().numpy().reshape(1,-1)[0]\n",
        "            A_k = batch_advs\n",
        "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "            #print('v', V.cpu().detach().numpy().reshape(1,-1)[0][-10:])\n",
        "            #print('adv', A_k)\n",
        "\n",
        "            # This is the loop where we update our network for some n epochs\n",
        "            for epoch in range(self.n_updates_per_iteration):\n",
        "\n",
        "\n",
        "                continue_training=True\n",
        "                approx_kl_divs=[]\n",
        "                for  _ in range(int(len(batch_acts)/self.batch)):\n",
        "                    indices = np.random.permutation(len(batch_acts))\n",
        "                    idx=indices[ :self.batch]\n",
        "                    batch_acts2=batch_acts[idx]\n",
        "                    batch_sine2=batch_sine[idx]\n",
        "                    batch_cosi2=batch_cosi[idx]\n",
        "                    batch_log_probs2=batch_log_probs[idx]\n",
        "                    batch_rtgs2=batch_rtgs[idx]\n",
        "                    A_k2=A_k[idx]\n",
        "\n",
        "\n",
        "                    # Calculate V_phi and pi_theta(a_t | s_t)\n",
        "                    V2, curr_log_probs,entropy = self.evaluate(batch_sine2,batch_cosi2, batch_acts2)\n",
        "\n",
        "                    ratios = curr_log_probs / batch_log_probs2 #torch.exp(curr_log_probs - batch_log_probs2)\n",
        "\n",
        "                    # Calculate surrogate losses.\n",
        "\n",
        "                    surr1 = ratios * A_k2\n",
        "                    surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k2\n",
        "\n",
        "                    #sum(-1)???\n",
        "                    actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                    entropy_loss=0 #self.entropy_w*(-torch.mean(entropy))\n",
        "\n",
        "                    critic_loss = self.critic_w*nn.L1Loss()(V2, batch_rtgs2)\n",
        "\n",
        "                    total_loss=actor_loss+entropy_loss+critic_loss\n",
        "\n",
        "                    with torch.no_grad():  #????\n",
        "                        log_ratio = curr_log_probs - batch_log_probs2\n",
        "                        approx_kl_div = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
        "                        approx_kl_divs.append(approx_kl_div)\n",
        "\n",
        "                    if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                        continue_training = False\n",
        "\n",
        "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
        "                        #break\n",
        "\n",
        "                    # Calculate gradients and perform backward propagation for critic network\n",
        "\n",
        "                    self.optima.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    #torch.nn.utils.clip_grad_norm(self.Actor_Critic.parameters(), 5)\n",
        "                    self.optima.step()\n",
        "\n",
        "                    self.optimc.zero_grad()\n",
        "                    critic_loss.backward()\n",
        "                    #torch.nn.utils.clip_grad_norm(self.Actor_Critic.parameters(), 5)\n",
        "                    self.optimc.step()\n",
        "\n",
        "                    # Log actor loss\n",
        "                    self.logger['actor_losses'].append(actor_loss.detach().numpy())\n",
        "                    #self.logger['entropy_losses'].append(entropy_loss.detach())\n",
        "                    self.logger['critic_losses'].append(critic_loss.detach().numpy())\n",
        "\n",
        "                #if not continue_training:\n",
        "                #    break\n",
        "\n",
        "            V, _ ,_= self.evaluate(batch_sine, batch_cosi,batch_acts)\n",
        "            #print(V.cpu().detach().numpy().reshape(1,-1)[0][-10:])\n",
        "\n",
        "            #lr =max(self.lr_inicial * (1-i_so_far/total_timesteps),0.0003)\n",
        "            #for param_group in self.optim.param_groups:\n",
        "            #    param_group['lr'] = lr\n",
        "\n",
        "\n",
        "            # Print a summary of our training so far\n",
        "            self.logger['approx_kl_divs'].append(np.mean(approx_kl_divs))\n",
        "\n",
        "            self._log_summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rollout(self):\n",
        "        \"\"\"\n",
        "            Too many transformers references, I'm sorry. This is where we collect the batch of data\n",
        "            from simulation. Since this is an on-policy algorithm, we'll need to collect a fresh batch\n",
        "            of data each time we iterate the actor/critic networks.\n",
        "            Parameters:\n",
        "                None\n",
        "            Return:\n",
        "                batch_obs - the observations collected this batch. Shape: (number of timesteps, dimension of observation)\n",
        "                batch_acts - the actions collected this batch. Shape: (number of timesteps, dimension of action)\n",
        "                batch_log_probs - the log probabilities of each action taken this batch. Shape: (number of timesteps)\n",
        "                batch_rtgs - the Rewards-To-Go of each timestep in this batch. Shape: (number of timesteps)\n",
        "                batch_lens - the lengths of each episode this batch. Shape: (number of episodes)\n",
        "        \"\"\"\n",
        "        # Batch data. For more details, check function header.\n",
        "        self.Actor.eval()\n",
        "        self.Critic.eval()\n",
        "\n",
        "        batch_acts = []\n",
        "        batch_log_probs = []\n",
        "        batch_rews = []\n",
        "        batch_rtgs = np.array([])\n",
        "        batch_lens = []\n",
        "        batch_value=[]\n",
        "        batch_sine=[]\n",
        "        batch_cosi=[]\n",
        "        dones=[]\n",
        "        batch_advs=np.array([])\n",
        "\n",
        "\n",
        "        # Episodic data. Keeps track of rewards per episode, will get cleared\n",
        "        # upon each new episode\n",
        "        ep_rews = []\n",
        "\n",
        "        t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
        "\n",
        "\n",
        "        # Keep simulating until we've run more than or equal to specified timesteps per batch\n",
        "        while t < self.timesteps_per_batch:\n",
        "            self.random=np.random.rand()\n",
        "            print(self.random)\n",
        "            ep_rews2 = [] # rewards collected per episode\n",
        "\n",
        "            # Reset the environment. sNote that obs is short for observation.\n",
        "            sine,cosi,done = env.env_start()\n",
        "\n",
        "            X=[]\n",
        "            Y=[]\n",
        "            ep_t=0\n",
        "            # Run an episode for a maximum of max_timesteps_per_episode timesteps\n",
        "            #for ep_t in range(self.max_timesteps_per_episode):\n",
        "            while not done:\n",
        "                ep_t+=1\n",
        "                # If render is specified, render the environment\n",
        "                if self.render and (self.logger['i_so_far'] % self.render_every_i == 0) and len(batch_lens) == 0:\n",
        "                    self.env.render()\n",
        "\n",
        "                #t += 1 # Increment timesteps ran this batch so far\n",
        "                t+=1\n",
        "\n",
        "                # Track observations in this batch\n",
        "\n",
        "                #obs = torch.from_numpy(np.array([obs])).float().to(device)\n",
        "                sine=torch.from_numpy(np.array([[sine]])).float()\n",
        "                cosi=torch.from_numpy(np.array([[cosi]])).float()\n",
        "\n",
        "\n",
        "                batch_sine.append(sine)\n",
        "                batch_cosi.append(cosi)\n",
        "\n",
        "\n",
        "                # Calculate action and make a step in the env.\n",
        "                # Note that rew is short for reward.\n",
        "                action, log_prob,value,_= self.get_action(sine,cosi)\n",
        "                rew,sine,cosi,done=env.env_step(action)\n",
        "                #obs,rew,done,_= self.env.step(action.flatten())\n",
        "\n",
        "                # Track recent reward, action, and action log probability\n",
        "                ep_rews.append(rew)\n",
        "                ep_rews2.append(rew)\n",
        "                batch_acts.append(action)\n",
        "                batch_value.append(value.detach().numpy()[0])\n",
        "                batch_log_probs.append(log_prob)\n",
        "                X.append(self.env.agent_loc[0])\n",
        "                Y.append(self.env.agent_loc[1])\n",
        "\n",
        "                # If the environment tells us the episode is terminated, break\n",
        "                if done:\n",
        "                    dones.append(1)\n",
        "                    break\n",
        "                dones.append(0)\n",
        "\n",
        "\n",
        "            # Track episodic lengths and rewards\n",
        "            batch_lens.append(ep_t + 1)\n",
        "            #batch_rews.append(ep_rews2)\n",
        "            self.logger['batch_rews'].append(ep_rews2)\n",
        "            plt.imshow(self.env.grid, cmap='gray')\n",
        "            plt.scatter(Y,X,color='r')\n",
        "            plt.scatter(self.env.goal[1],self.env.goal[0],color='b')\n",
        "            plt.scatter(self.env.agent_loc[1],self.env.agent_loc[0],color='g')\n",
        "            plt.show()\n",
        "            print(batch_value[-len(ep_rews2):])\n",
        "\n",
        "\n",
        "            batch_advs2,batch_rtgs2 = self.compute_advs(ep_rews2,batch_value[-len(ep_rews2):])\n",
        "            print('return',batch_rtgs2)\n",
        "            print('gae',batch_advs2)\n",
        "            batch_advs=np.append(batch_advs,batch_advs2)\n",
        "            batch_rtgs=np.append(batch_rtgs,batch_rtgs2)\n",
        "\n",
        "        #batch_value.append(0)\n",
        "\n",
        "\n",
        "        # Reshape data as tensors in the shape specified in function description, before returning\n",
        "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "\n",
        "\n",
        "        batch_log_probs=torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "\n",
        "        batch_sine=torch.tensor(np.vstack(batch_sine), dtype=torch.float)\n",
        "        batch_cosi=torch.tensor(np.vstack(batch_cosi), dtype=torch.float)\n",
        "\n",
        "        #batch_rews2=np.array(batch_rews)/(np.max(ep_rews)+1e-5)\n",
        "        #batch_rews2=torch.tensor(batch_rews, dtype=torch.float)\n",
        "\n",
        "        #torch.from_numpy(np.vstack(ep_rews)/(np.max(ep_rews)+1e-5)).float().to(device)\n",
        "\n",
        "        dones=torch.tensor(dones, dtype=torch.float)\n",
        "        #print(batch_advs)\n",
        "\n",
        "        batch_advs=torch.tensor(np.vstack(batch_advs), dtype=torch.float)   #torch.tensor(np.vstack(batch_advs), dtype=torch.float)\n",
        "        batch_rtgs=torch.tensor(np.vstack(batch_rtgs), dtype=torch.float)\n",
        "\n",
        "        #self.logger['batch_lens'].append([np.sum(ep_rews) for ep_rews in self.logger['batch_rews']])\n",
        "\n",
        "        self.Actor.train()\n",
        "        self.Critic.train()\n",
        "\n",
        "        return batch_sine, batch_cosi, batch_acts, batch_log_probs, batch_rtgs, batch_lens,dones,batch_advs\n",
        "\n",
        "\n",
        "\n",
        "    def compute_advs(self, batch_rews,batch_value):\n",
        "        with torch.no_grad():\n",
        "            advantages = []\n",
        "            returns = []\n",
        "\n",
        "            size=0\n",
        "\n",
        "            aux=[]\n",
        "            last_gae_lam = 0\n",
        "\n",
        "            next_non_terminal=next_values=0\n",
        "            returne=0\n",
        "\n",
        "            for rew in reversed(range(len(batch_rews))):\n",
        "\n",
        "\n",
        "                returne=batch_rews[rew] + self.gamma * next_values * next_non_terminal\n",
        "\n",
        "                delta = batch_rews[rew] + self.gamma * next_values * next_non_terminal - batch_value[rew]\n",
        "                #last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
        "                advantages.insert(0, delta )   #VERIFICAR GAE\n",
        "                returns.insert(0,returne)\n",
        "                next_non_terminal = 1\n",
        "\n",
        "                next_values = batch_value[rew]\n",
        "\n",
        "        return advantages,returns\n",
        "\n",
        "\n",
        "    def get_action(self,sine,cosi,train=True ):\n",
        "        \"\"\"\n",
        "            Queries an action from the actor network, should be called from rollout.\n",
        "            Parameters:\n",
        "                obs - the observation at the current timestep\n",
        "            Return:\n",
        "                action - the action to take, as a numpy array\n",
        "                log_prob - the log probability of the selected action in the distribution\n",
        "        \"\"\"\n",
        "        # Query the actor network for a mean action\n",
        "        if np.random.rand()<self.random and train:\n",
        "          #print(self.env.goal,self.env.agent_loc)\n",
        "          #if direction>0 and direction<=np.pi/2:   #1quadrante\n",
        "\n",
        "           # action=np.random.choice([3,0],1)\n",
        "            #print(direction,'1',action)\n",
        "          #elif direction>np.pi/2 and direction<=np.pi: #2quadrante\n",
        "\n",
        "           # action=np.random.choice([1,0],1)\n",
        "            #print(direction,'2',action)\n",
        "          #elif direction<0 and direction>=-np.pi/2: #4quadrante\n",
        "\n",
        "          #  action=np.random.choice([3,2],1)\n",
        "            #print(direction,'4',action)\n",
        "          #else:  #3quadrante\n",
        "\n",
        "           # action=np.random.choice([2,1],1)\n",
        "            #print(direction,'3',action)\n",
        "          action=np.random.choice([0,3,2,1],1)\n",
        "          action=torch.from_numpy(action)\n",
        "          action,log_prob,entropy,dist = self.Actor.action(sine,cosi,action)\n",
        "          value = self.Critic(sine,cosi)\n",
        "\n",
        "        else:\n",
        "          action,log_prob,entropy,dist = self.Actor(sine,cosi)\n",
        "          value = self.Critic(sine,cosi)\n",
        "\n",
        "          #log_prob = dist.log_prob(action)\n",
        "        action=action.detach().numpy()\n",
        "        log_prob = log_prob.detach().numpy()\n",
        "\n",
        "\n",
        "        return action, log_prob,value[0],dist\n",
        "\n",
        "    def play(self):\n",
        "        self.Actor.eval()\n",
        "        self.Critic.eval()\n",
        "        sine,cosi,done = env.env_start()\n",
        "        print(env.agent_loc)\n",
        "        while not done:\n",
        "\n",
        "            sine=np.array([[sine]])\n",
        "            cosi=np.array([[cosi]])\n",
        "            sine=torch.from_numpy(sine).float()\n",
        "            cosi=torch.from_numpy(cosi).float()\n",
        "\n",
        "            action, log_prob,value,dist= self.get_action(sine,cosi,train=False)\n",
        "            rew,sine,cosi,done=env.env_step(action)\n",
        "            print(action,dist,value,rew,log_prob)\n",
        "            plt.imshow(self.env.grid, cmap='gray')\n",
        "            plt.scatter(self.env.goal[1],self.env.goal[0],color='b')\n",
        "            plt.scatter(self.env.agent_loc[1],self.env.agent_loc[0],color='g')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self,batch_sine, batch_cosi, batch_acts):\n",
        "        \"\"\"\n",
        "            Estimate the values of each observation, and the log probs of\n",
        "            each action in the most recent batch with the most recent\n",
        "            iteration of the actor network. Should be called from learn.\n",
        "            Parameters:\n",
        "                batch_obs - the observations from the most recently collected batch as a tensor.\n",
        "                            Shape: (number of timesteps in batch, dimension of observation)\n",
        "                batch_acts - the actions from the most recently collected batch as a tensor.\n",
        "                            Shape: (number of timesteps in batch, dimension of action)\n",
        "            Return:\n",
        "                V - the predicted values of batch_obs\n",
        "                log_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
        "        \"\"\"\n",
        "        # Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
        "        action,log_prob,entropy,_ = self.Actor(batch_sine, batch_cosi)\n",
        "        value = self.Critic(batch_sine, batch_cosi)\n",
        "\n",
        "        #log_prob = dist.log_prob(batch_acts.squeeze())\n",
        "\n",
        "\n",
        "        return value.reshape(1,-1)[0], log_prob,entropy\n",
        "\n",
        "\n",
        "    def _init_hyperparameters(self, hyperparameters):\n",
        "        \"\"\"\n",
        "            Initialize default and custom values for hyperparameters\n",
        "            Parameters:\n",
        "                hyperparameters - the extra arguments included when creating the PPO model, should only include\n",
        "                                    hyperparameters defined below with custom values.\n",
        "            Return:\n",
        "                None\n",
        "        \"\"\"\n",
        "        # Initialize default values for hyperparameters\n",
        "        # Algorithm hyperparameters\n",
        "        self.timesteps_per_batch = 1000                 # Number of timesteps to run per batch\n",
        "        self.max_timesteps_per_episode = 600           # Max number of timesteps per episode\n",
        "        self.n_updates_per_iteration = 10                # Number of times to update actor/critic per iteration\n",
        "        self.lr = 0.001                                 # Learning rate of actor optimizer\n",
        "        self.gamma = 0.99                               # Discount factor to be applied when calculating Rewards-To-Go\n",
        "        self.clip = 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
        "\n",
        "        # Miscellaneous parameters\n",
        "        self.render = False                              # If we should render during rollout\n",
        "        self.render_every_i = 10                        # Only render every n iterations\n",
        "        self.save_freq = 10                             # How often we save in number of iterations\n",
        "        self.seed = None                                # Sets the seed of our program, used for reproducibility of results\n",
        "\n",
        "        # Change any default values to custom values for specified hyperparameters\n",
        "        for param, val in hyperparameters.items():\n",
        "            exec('self.' + param + ' = ' + str(val))\n",
        "\n",
        "        # Sets the seed if specified\n",
        "        if self.seed != None:\n",
        "            # Check if our seed is valid first\n",
        "            assert(type(self.seed) == int)\n",
        "\n",
        "            # Set the seed\n",
        "            torch.manual_seed(self.seed)\n",
        "            print(f\"Successfully set seed to {self.seed}\")\n",
        "\n",
        "    def _log_summary(self):\n",
        "        \"\"\"\n",
        "            Print to stdout what we've logged so far in the most recent batch.\n",
        "            Parameters:\n",
        "                None\n",
        "            Return:\n",
        "                None\n",
        "        \"\"\"\n",
        "        # Calculate logging values. I use a few python shortcuts to calculate each value\n",
        "        # without explaining since it's not too important to PPO; feel free to look it over,\n",
        "        # and if you have any questions you can email me (look at bottom of README)\n",
        "        delta_t = self.logger['delta_t']\n",
        "        self.logger['delta_t'] = time.time()\n",
        "        delta_t = (self.logger['delta_t'] - delta_t) / 1e9\n",
        "        delta_t = str(round(delta_t, 2))\n",
        "\n",
        "        t_so_far = self.logger['t_so_far']\n",
        "        i_so_far = self.logger['i_so_far']\n",
        "        avg_ep_lens = np.mean(self.logger['batch_lens'][-100:])\n",
        "        avg_ep_rews = np.mean([np.sum(ep_rews) for ep_rews in self.logger['batch_rews']][-100:])\n",
        "        max_ret=np.max([np.sum(ep_rews) for ep_rews in self.logger['batch_rews']])\n",
        "        avg_actor_loss = np.mean([np.mean(self.logger['actor_losses'])])\n",
        "        avg_critic_loss = np.mean([np.mean(self.logger['critic_losses'])])\n",
        "        avg_entropy_loss = np.mean([np.mean(self.logger['entropy_losses'])])\n",
        "\n",
        "\n",
        "\n",
        "        self.avg_ret.append(np.round(avg_ep_rews,2))\n",
        "        # Round decimal places for more aesthetic logging messages\n",
        "        avg_ep_lens = str(round(avg_ep_lens, 2))\n",
        "        avg_ep_rews = str(round(avg_ep_rews, 2))\n",
        "        avg_actor_loss = str(round(avg_actor_loss, 5))\n",
        "\n",
        "        # Print logging statements\n",
        "        print(flush=True)\n",
        "        print(f\"-------------------- Iteration #{i_so_far} --------------------\", flush=True)\n",
        "        print(f'random ',self.random)\n",
        "        print(f\"Average : {self.lr_inicial}\", flush=True)\n",
        "        print(f\"Average Episodic Return: {avg_ep_rews}\", flush=True)\n",
        "        print(f\"Max Episodic Return: {max_ret}\", flush=True)\n",
        "        print(f\"Average Loss: {avg_actor_loss}\", flush=True)\n",
        "        print(f\"Average Loss Critic: {avg_critic_loss}\", flush=True)\n",
        "        #print(f\"STD: {std}\", flush=True)\n",
        "        print(f\"TEntropy: {avg_entropy_loss}\", flush=True)\n",
        "        print(f\"approx_kl_divs: {self.logger['approx_kl_divs']} secs\", flush=True)\n",
        "        print(f\"------------------------------------------------------\", flush=True)\n",
        "        print(flush=True)\n",
        "\n",
        "        # Reset batch-specific logging data\n",
        "        #self.logger['batch_rews'] = []\n",
        "\n",
        "        self.logger['approx_kl_divs']=[]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "i_sHRJ5lN5OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train easy mode\n",
        "img_matrix_filtered2=np.ones((6,6))*125\n",
        "startlat=1\n",
        "startlon=1\n",
        "endlon=5\n",
        "endlat=5"
      ],
      "metadata": {
        "id": "6gekeShROJZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env=Environment()\n",
        "env_info={'grid':img_matrix_filtered2,'grid_h':img_matrix_filtered2.shape[0],'grid_w':img_matrix_filtered2.shape[1],\n",
        "          'size':5,'start':[startlat,startlon],'end':[endlat,endlon]}\n",
        "env.env_init(env_info)"
      ],
      "metadata": {
        "id": "B_CwlCogOJWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "                'gamma': 0.98 ,\n",
        "                'gae_lambda':0.8,\n",
        "                'lr': 0.01,\n",
        "                'timesteps_per_batch':50,\n",
        "                'max_timesteps_per_episode':500,\n",
        "                'total_timesteps':100,\n",
        "                'n_updates_per_iteration':5,\n",
        "                'target_update_interval':1,\n",
        "                'clip': 0.2,\n",
        "                'render': False,\n",
        "                'render_every_i': 10\n",
        "              }\n",
        "agent=PPO(env,6*6, 4, random_seed=10,**hyperparameters)"
      ],
      "metadata": {
        "id": "TvVRZ4sKOJUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.learn(500)"
      ],
      "metadata": {
        "id": "3TQ7QAoOOJSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(agent.logger['actor_losses'],'.' )"
      ],
      "metadata": {
        "id": "bLS7NnsMORjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.play()"
      ],
      "metadata": {
        "id": "YYYoEFroORf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.Actor_Critic.state_dict(),'weihts.pth')"
      ],
      "metadata": {
        "id": "sSewWfgSORd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.Actor_Critic.state_dict()"
      ],
      "metadata": {
        "id": "yJNsIKp6ObDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}